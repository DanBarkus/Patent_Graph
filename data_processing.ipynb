{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Patent Graph</h1>\n",
    "\n",
    "We're going to explore what it looks like to explore and load in an external dataset (US Patents) into TigerGraph. We'll cover all aspects needed to get started with setting up your first graph from understanding your data, to building a schema based on it, to processing it to make sure it conforms to our loading standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "We'll be using a dataset from the US Patent Office available [here](https://www.uspto.gov/ip-policy/economic-research/research-datasets/patent-examination-research-dataset-public-pair). \n",
    "\n",
    "<img src=\"images/data.png\" width=\"500px\"/>\n",
    "\n",
    "Select one of the above data files in **.csv format**. All of the years follow the same format, but differ in the amount of data that they have. Once downloaded, the data will decompress to about **6x** the size listed on the site. Keep this in mind as you will have to upload this data at some point to your TigerGraph server and if you have a slower network connection, the smaller size might be preferred. Additionally, the Free tier TigerGraph instance is limited to 50gb of disk space, so you may run into that limitation with the 2020 and 2019 data sets.\n",
    "\n",
    "I'm not including the full dataset in this repo, but will include the top 10K lines of each file so that you can follow along while your full dataset downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Data Manageable\n",
    "\n",
    "It's a lot easier to work with smaller files when we're setting up our schema. Depending on how thorough you are with your initial data investigation or how well you know your dataset, you most likely won't nail your schema on the first shot. Sometimes this will involve having to change the format or re-structure the data files. That also means re-uploading the new file to your TigerGraph server. It's a lot easier and faster to do this with a ~10MB file than a ~11GB one.\n",
    "\n",
    "The below function will take the top `numLines` lines from each data file in the patent dataset and create a new file following the same naming convention as the original file, but with `10K_` prepended to the name of the file.\n",
    "\n",
    "If you are using the `10K_` files from the `processed_data` folder while you wait for your full files to download, then you **do not** need to run this cell. It is important to understand the importance of pairing down your data during the exploration phase, but you can use the pre-prepared files provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# specify data folders\n",
    "data_folder = \"./raw_data\"\n",
    "output_folder = \"./processed_data\"\n",
    "\n",
    "# How many lines do we want in the stripped down files\n",
    "numLines = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all the files in the data folder\n",
    "for root, dirs, files in os.walk(data_folder):\n",
    "    # Make the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    for fi in files:\n",
    "        filepath = os.path.join(root,fi)\n",
    "        # Create an output file with '10K_' before the file name (change this if you're using a different # than 10,000)\n",
    "        outputFile = open(os.path.join(output_folder,(\"10K_\"+fi)),'w+')\n",
    "        # for each datafile\n",
    "        with open(filepath) as dataFile:\n",
    "            # go through each line in the file until we hit numLines\n",
    "            try: \n",
    "                head = [next(dataFile) for x in range(numLines)]\n",
    "                outputFile.writelines(head)\n",
    "            # This will trigger if the file is less than numLines and will just copy the whole file\n",
    "            except StopIteration:\n",
    "                dataFile = open(filepath)\n",
    "                lines = dataFile.readlines()\n",
    "                outputFile.writelines(lines)\n",
    "                outputFile.close()\n",
    "        outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How TigerGraph works under the Hood\n",
    "\n",
    "One key step in the thought process of how we want to structure our data is knowing how TigerGraph handles the data that has been loaded into it. There are three key components in a Graph Database. \n",
    "\n",
    "- **Vertex** (aka node) - represent an individual entry of a particular data concept (ex. Person, Receipt, House)\n",
    "- **Edge** - link together vertices via relationships (ex. owns_vehicle, is_friend, purchased_item)\n",
    "- **Attribute** - describes additional information about a **Vertex** or **Edge** (ex. firstName, streetNumber, transactionDate)\n",
    "\n",
    "The data is stored on the TigerGraph server as follows:\n",
    "\n",
    "**Vertex and Edge** - The `primary_id` of all vertices is loaded in **memory** and an edge is stored pointing between memory locations. This makes traversing edges *extremely* fast.\n",
    "\n",
    "**Attributes** - All other details of **Vertices and Edges** are read from disk and accessing them will incur slight performance overhead above just traversing edges. You can add one **Attribute** as an **Index** per vertex type to increase performance accessing that **attribute**, but this is advanced and we won't be covering that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "\n",
    "Now that we have paired down versions of our files, let's take a look at what we're actually working with. Below we'll print out each file name as well as its header. As you look through each file, try to figure out what will be our **Vertices**, **Edges**, and **Attributes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10K_all_inventors.csv\n",
      "application_number,inventor_name_first,inventor_name_middle,inventor_name_last,inventor_rank,inventor_city_name,inventor_region_code,inventor_country_code\n",
      " 04840815,WILLIAM,D.,SCHAEFFER,1,POMONA,CA,US\n",
      "\n",
      "10K_transactions.csv\n",
      "application_number,event_code,recorded_date\n",
      " 02549302,SETS,2001-09-20\n",
      "\n",
      "10K_application_data.csv\n",
      "application_number,filing_date,application_invention_type,examiner_full_name,examiner_art_unit,uspc_class,uspc_subclass,confirm_number,atty_docket_number,appl_status_desc,appl_status_date,file_location,file_location_date,earliest_pgpub_number,earliest_pgpub_date,wipo_pub_number,wipo_pub_date,patent_number,patent_issue_date,invention_title,small_entity_indicator,aia_first_to_file\n",
      " 04453098,,,\"LATEEF, MARVIN M\",2106,338,254000,6933,,Patented File - (Old Case Added for File Tracking Purposes),1983-12-28,FILE REPOSITORY (FRANCONIA),1986-04-23,,,,,,,,UNDISCOUNTED,\n",
      "\n",
      "10K_event_codes.csv\n",
      "event_code,description\n",
      " 102P,102P\n",
      "\n",
      "10K_pte_summary.csv\n",
      "application_number,pto_adjustment,pto_delay,applicant_delay,patent_term_extension\n",
      " 09068213,0,0,0,0\n",
      "\n",
      "10K_attorney_agent.csv\n",
      "atty_name_first,atty_name_last,atty_name_middle,atty_name_suffix,atty_phone_number,atty_registration_number,atty_practice_category,application_number\n",
      " James,Wetzel,,,,17686,Attorney,03831599\n",
      "\n",
      "10K_foreign_priority.csv\n",
      "application_number,foreign_parent_id,foreign_parent_date,parent_country\n",
      " 08030312,2297/90,1990-09-24,DENMARK\n",
      "\n",
      "10K_pat_term_adj.csv\n",
      "application_number,pta_sequence_number,pta_event_date,pta_event_desc,applicant_delay_duration,uspto_delay_duration,start_pta_sequence_number,term_extension_indicator\n",
      " 09068213,,2002-12-02,Mail Notice of Allowance,0,0,,1\n",
      "\n",
      "10K_continuity_children.csv\n",
      "application_number,child_application_number,child_filing_date\n",
      " 02262037,59997901,2018-01-01\n",
      "\n",
      "10K_correspondence_address.csv\n",
      "application_number,correspondence_name,correspondence_street_line_1,correspondence_street_line_2,correspondence_street_line_3,correspondence_city,correspondence_postal_code,correspondence_region_code,correspondence_country_code,customer_number\n",
      " 02000161,,,,,,,,,\n",
      "\n",
      "10K_pta_summary.csv\n",
      "application_number,pto_delay_a,pto_delay_b,pto_delay_c,overlap_pto_delay,nonoverlap_pto_delay,pto_manual_adjustment,applicant_delay,patent_term_adjustment\n",
      " 09743549,0,0,0,0,0,0,0,0\n",
      "\n",
      "10K_continuity_parents.csv\n",
      "application_number,parent_application_number,parent_filing_date,continuation_type\n",
      " 05354590,05101449,1970-12-28,CON\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# go through the files in the output folder\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for fi in files:\n",
    "        filepath = os.path.join(root,fi)\n",
    "        with open(filepath) as dataFile:\n",
    "            # Print filename and first 2 lines (header + 1st row of data)\n",
    "            print(fi)\n",
    "            print(dataFile.readline(),dataFile.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_inventors.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`all_inventors` as the name implies, contains information about the inventors who will be mentioned in the patent applications in the dataset.\n",
    "\n",
    "This is ONE TABLE but I have split it to better fit most screens. Any future entries will also represent ONE TABLE unless otherwise specified.\n",
    "\n",
    "|application_number|inventor_name_first|inventor_name_middle|inventor_name_last|inventor_rank|\n",
    "|---|---|---|---|---|\n",
    "|04840815|WILLIAM|D.|SCHAEFFER|1|\n",
    "\n",
    "<br>\n",
    "\n",
    "|inventor_city_name|inventor_region_code|inventor_country_code|\n",
    "|---|---|---|\n",
    "|POMONA|CA|US|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, we can tell that this list is referencing inventors back to their patent applications. `application_number` is the primary id that the patent applications use and that number ties each one of these inventors back to a patent application. \n",
    "Additionally, we see that the rest of the fields are describing an Inventor. From this first file, we can gather that `application_number` is something that we want to look for in the other files, and that there are a list of attributes that describe Inventor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building our schema based on what we know.\n",
    "\n",
    "First we need to identify the objects that this file talks about. The first immediate one is **Inventor**. Additionally we know that **Applications** exist due to the reference to `application_number`. In addition to just knowing that **Inventor** exists, we also know a little bit about our **Inventor**s such as their first, middle, and last names as well as the region that they live in. One thing that we do not have for our **Inventor**s is a unique identifier. There's no `inventor_id` or other field that could be used to ensure unique inventors. This is frustrating, but real-life data isn't perfect, so we'll need to generate ourselves. We'll get into that later on though, let's start with the basics first.\n",
    "\n",
    "We can use the column names to define our first Vertices in the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-**Inventor**\n",
    " - id (we have to generate this)\n",
    " - name_first\n",
    " - name_middle\n",
    " - name_last\n",
    " - inventor_rank\n",
    " - inventor_city\n",
    " - inventor_region\n",
    " - inventor_country\n",
    "\n",
    "-**Application**\n",
    " - application_number\n",
    " (That's all we know for now about applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at what we have above we can see that our **Inventor** is actually describing 3 things. The **Inventor** themselves (names), their **Rank** on the patent, and the **Location** that they used at the time of the filing.\n",
    "\n",
    "This is where our domain knowledge will come in a little bit. When filing a patent, the inventors can be ordered by how much they contributed to the patent. The order of a name in the patent listing is their **Rank**. This **Rank** can be unique across each **Application** that an **Inventor** is on. Because of that, it does not make sense to store **Rank** inside of **Inventor** because that will only reflect one particular **Application**.\n",
    "\n",
    "So what do we do here? Let's walk through the possibilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wrong Way\n",
    "\n",
    "The first one is that we break off **Rank** as its own Vertex. This seems logical, because an **Inventor** `has_rank` **Rank**. But now let's run this through a theoretical example.\n",
    "\n",
    "<img src=\"images/rank_schema.png\" width=\"500\"/>\n",
    "\n",
    "*Inventor 1* has filed two **Application**s, *Application 1* and *Application 2*. *Inventor 1* is *Rank 1* on the first application and *Rank 2* on the second application. Following the solution outlined above, *Inventor 1* would have two **Rank** vertices attached to them, *Rank 1* and *Rank 2*. However, there's nothing that would tie either *Rank 1* or *Rank 2* to a particular **Application**. So finding the **Rank** of *Inventor 1* on *Application 1* would return both *Rank 1* and *Rank 2*. \n",
    "\n",
    "<img src=\"images/rank_1.png\" width=\"300\"/>\n",
    "<img src=\"images/rank_1_1.png\" width=\"300\"/>\n",
    "\n",
    "Okay then, so let's also tie our **Rank** to **Application**. Now *Inventor 1* `has_application` *Application 1*. *Application 1* `has_inventor_rank' *Rank 1* and *Rank 1* also ties back to *Inventor 1*\n",
    "\n",
    "<img src=\"images/rank_2_1.png\" width=\"500\"/>\n",
    "\n",
    "This seems like it would work (it won't), so let's see how messy this gets when we consider multiple **Application**s. Every **Application** will 'has_inventor_rank' *Rank 1* because there has to be at lest one **Inventor** on an **Application**. So if we wanted to find out what **Rank** an **Inventor** was in any given **Application**, our traversal would still leave us with an open answer. Let's take a look.\n",
    "\n",
    "We would start at an **Inventor**, then follow the `filed_application` edge to **Application 1**. From there, we would do a multi-hop traversal from **Application 1** via `has_inventor_rank` to a **Rank** vertex, then through `has_rank` back to our source **Inventor**. \n",
    "\n",
    "As you might already see, we run into an issue if **Application 1** has a second inventor of **Rank 2** and the source **Inventor** happens to be **Rank 2** on *any* other application. Because those **Rank** vertices are connected to multiple **Applications** and **Inventors** it is impossible to distinguish which instance of `has_rank` corresponds to any given instance of `has_inventor_rank`.\n",
    "\n",
    "<img src=\"images/rank_2_2.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Right Way\n",
    "\n",
    "Luckily, there's a much easier way than all of this. We don't have to limit our information to only our Vertices, we can also store additional data along edges.\n",
    "\n",
    "<img src=\"images/rank_3_1.png\" width=\"500\"/>\n",
    "\n",
    "Instead of making a Vertex for **Rank**, we can include it as an attribute of the `filed_application` edge. Now, all we need to do is traverse one edge in order to find out not only which **Application** an **Inventor** filed, but also their **Rank** on that application.\n",
    "\n",
    "<img src=\"images/rank_3_2.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Location** can and should be separated into its own **Vertex**. Unlike **Rank**, **Location** directly relates to just the **Inventor**. Because of that, a single edge can be used to connect **Inventor** -> `from_location` -> **Location**.\n",
    "\n",
    "It is also important to note that we may want to **filter** on location later. Say, select all **Inventors** from a specific **Location**. When you anticipate wanting to filter on a concept like that, it's a good indicator that that concept should be a **Vertex**.\n",
    "\n",
    "But we can go deeper here. **Location** contains three pieces of information, **Country**, **Region**, and **City**. Each of those sound like something we might want to filter on down the line... see where I'm going here?\n",
    "\n",
    "What was once a single **Location** vertex can now be described as 3 vertices: **Country**, **Region**, and **City**. We further know from our sample data that **Region** is a US State and that is inside **Country** and **City** is inside **Region**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to take a second to state why its so advantageous to separate anything that you want to filter on into its own **Vertex**. To do this we need to think about how we query data.\n",
    "\n",
    "Here's the theoretical example: We want to select all Inventors who filed in the **City** of **Boston**\n",
    "\n",
    "If *city* is an *attribute* on an **Inventor**, then in order to find all **Inventors** whose *city* = `Boston` we need to:\n",
    "- check EVERY SINGLE **Inventor**\n",
    "- read the *city* attribute\n",
    "- compare that value to `Boston`\n",
    "- return **Inventors** with matching value\n",
    "\n",
    "If **City** is a **Vertex** connected to an **Inventor**, then we just need to find all **Inventors** connected via a `from_city` edge to the **Boston** vertex.\n",
    "- select **Boston**\n",
    "- traverse `from_city` (pointer in memory)\n",
    "- return **Inventors** at resulting memory locations\n",
    "\n",
    "The **TL;DR** is you have to touch EVERY **Inventor** vertex to filter an *attribute*, and you ONLY touch the **Inventor** vertices you are interested in when you filter on a **Vertex**. Much more performant only grabbing `758` **Inventors** + the **Boston** vertex, than checking all `21,617,363` **Inventors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema so Far\n",
    "\n",
    "-**Inventor**\n",
    " - id (we have to generate this)\n",
    " - name_first\n",
    " - name_middle\n",
    " - name_last\n",
    "\n",
    "-**Country**\n",
    "\n",
    "-**Region**\n",
    "\n",
    "-**City**\n",
    "\n",
    "-**Application**\n",
    " - application_number\n",
    "\n",
    "<img src=\"images/inventors_schema.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transactions.csv\n",
    "\n",
    "|application_number|event_code|recorded_date|\n",
    "|---|---|---|\n",
    "|02549302|SETS|2001-09-20|\n",
    "\n",
    "**Transactions** are described in [appendix B](https://www.uspto.gov/sites/default/files/documents/Appendix%20B.pdf) for our dataset. At its most basic level, the `transactions` file just lists each event in the **Application**'s lifecycle. Each event, or **Transaction** is tied to an **Event Code** which describes the event and a date describing when the **Transaction** took place.\n",
    "\n",
    "Here we have to think about what a **Transaction** really is and what additional information is required to describe it.\n",
    "\n",
    "Here are two ways that we can represent a **Transaction**:\n",
    "\n",
    "<img src=\"images/transactions_schema.png\" width=\"700px\" />\n",
    "\n",
    "In the example on the left, the **Transaction** vertex only serves purpose to hold the *date* of the **Transaction**. To further complicate things, each **Vertex** needs a unique id. We don't have a unique id in the `transactions.csv` file. We could generate some, sure, but this still isn't the best answer. \n",
    "\n",
    "For example, if we want to find what event codes an application has, we need to traverse the `has_transaction` edge, then traverse the `has_code` edge. So, for each **Transaction** we need to make 2n hops where n = number of **Event Codes** related to the **Application**\n",
    "\n",
    "Looking at the example on the right, `date` is stored on an edge between **Application** and **Event Code** negating the need for the **Transaction** vertex. Since a **Transaction** in this sense is a relationship between an **Application** and an **Event Code**, it makes sense for it to be represented by an edge. \n",
    "\n",
    "Additinally, we only need to traverse 1n edges in this example to get the **Event Codes** related to a **Application**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### event_codes.csv\n",
    "\n",
    "|event_code|description|\n",
    "|---|---|\n",
    "|102P|102P|\n",
    "\n",
    "This one is pretty simple. It relates an **Event Code** to its corresponding, human readable *description*.\n",
    "\n",
    "With this and what we learned from `transactions.csv`, let's see what our schema looks like now.\n",
    "\n",
    "<img src=\"images/event_schema.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attorney_agent.csv\n",
    "\n",
    "|atty_name_first|atty_name_last|atty_name_middle|atty_name_suffix|atty_phone_number|\n",
    "|---|---|---|---|---|\n",
    "|James|Wetzel||||\n",
    "\n",
    "<br>\n",
    "\n",
    "|atty_registration_number|atty_practice_category|application_number|\n",
    "|---|---|---|\n",
    "|17686|Attorney|03831599|\n",
    "\n",
    "Hopefully this should start making sense by now. This file has a lot of columns, but is relatively simple.\n",
    "\n",
    "-**Attorney**\n",
    " - atty_number (primary_id)\n",
    " - first_name\n",
    " - last_name\n",
    " - middle_name\n",
    " - suffix\n",
    " - phone\n",
    "\n",
    "-**Practice Category**\n",
    "\n",
    "-**Application**\n",
    "\n",
    "**Practice Category** is broken off into its own vertex because it is a separate concept from **Attorney** and we might want to filter on it later.\n",
    "\n",
    "<img src=\"images/attorney_schema.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### application_data.csv\n",
    "\n",
    "|application_number|filing_date|application_invention_type|examiner_full_name|examiner_art_unit|uspc_class|\n",
    "|---|---|---|---|---|---|\n",
    "|04453098|||\"LATEEF, MARVIN M\"|2106|338|\n",
    "\n",
    "<br>\n",
    "\n",
    "|uspc_subclass|confirm_number|atty_docket_number|appl_status_desc|appl_status_date|file_location|\n",
    "|---|---|---|---|---|---|\n",
    "|254000|6933||Patented File - (Old Case Added for File Tracking Purposes)|1983-12-28|FILE REPOSITORY (FRANCONIA)|\n",
    "\n",
    "<br>\n",
    "\n",
    "|file_location_date|earliest_pgpub_number|earliest_pgpub_date|wipo_pub_number|wipo_pub_date|patent_number|\n",
    "|---|---|---|---|---|---|\n",
    "|1986-04-23|||||\n",
    "\n",
    "<br>\n",
    "\n",
    "|patent_issue_date|invention_title|small_entity_indicator|aia_first_to_file|\n",
    "|---|---|---|---|\n",
    "||||UNDISCOUNTED|\n",
    "\n",
    "This is the big one. It was so big I had to break down the table so it would fit on the screen. Luckily we've been practicing for this. It's time to put everything we've learned so far to use and make easy work of converting this file to schema.\n",
    "\n",
    "All the nuances of `application_data.csv` are outlined in [Appendix A](https://www.uspto.gov/sites/default/files/documents/Appendix%20A.pdf) of the dataset. But just like everything else, we'll walk through it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-**Application**\n",
    "- application_number (primary id)\n",
    "- filing_date\n",
    "- confirm_number\n",
    "- docket_number\n",
    "- invention_title\n",
    "\n",
    "-**Invention Type**\n",
    "\n",
    "-**Examiner**\n",
    "- full_name\n",
    "\n",
    "-**Art Unit**\n",
    "\n",
    "-**USPC Class**\n",
    "\n",
    "-**USPC Subclass**\n",
    "\n",
    "-**Application Status**\n",
    "\n",
    "-**File Location**\n",
    "\n",
    "-**Pgpub Number**\n",
    "\n",
    "-**WIPO Pub Number**\n",
    "\n",
    "-**Patent Number**\n",
    "\n",
    "-**Small Entity**\n",
    "\n",
    "-**First to File**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, that's a lot. And that's just the Nodes...\n",
    "\n",
    "Few things to talk about here, *confirm_number* and *docket_number* are just another reference number to the patent and therefor doesn't need thier own vertex.\n",
    "\n",
    "We will probably want to filter on **Invention Type**, **Application Status**, and **File Location** so those will be vertices.\n",
    "\n",
    "**Pgpub**, **WIPO**, and **Patent Number** are all concepts that could be attributes. However, each of these concepts has a date attached to it telling us when it happened. Because these don't necessarily happen on the same date as our **Application**, they can be considered separate concepts. We'll use an edge with the relevant date to tie them back to the **Application**.\n",
    "\n",
    "**Small Entity** and **First to File** are tags letting us know if this **Application** comes from a company with less than 500 people, and if this **Application** follows the AIA First to File rules respectively. Because we might want to filter on these, they're vertices.\n",
    "\n",
    "**Examiner** is someone who looks at a patent from a domain standpoint to make sure that it is unique and works with **Inventors** to assess if a **Patent** can be granted.\n",
    "\n",
    "**Art Unit** describes what unit the **Examiner** is in and we'll want to make that a vertex for filtering purposes.\n",
    "\n",
    "**USPC Class** and **USPC Subclass** both describe the classification of an **Application** and that's definitely something we want to be able to filter on, so vertex.\n",
    "\n",
    "<img src=\"images/application_schema.png\" width=\"500px\" />\n",
    "\n",
    "^^^ That's just this file. Our schema is starting to get complex. This is good, the more detail we capture in our schema, the more insights we will be able to extract from our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreign_priority.csv\n",
    "\n",
    "|application_number|foreign_parent_id|foreign_parent_date|parent_country|\n",
    "|---|---|---|---|\n",
    "|08030312|2297/90|1990-09-24|DENMARK|\n",
    "\n",
    "Let's do an easy one after that last one. [Appendix D](https://www.uspto.gov/sites/default/files/documents/Appendix%20D.pdf) describes **Foreign Parent** and the fields within.\n",
    "\n",
    "-**Foreign Parent**\n",
    "\n",
    "-**Parent Country**\n",
    "\n",
    "Here we're really only describing two concepts, the **Foreign Parent** the application from another country that is being referneced by our **Application**, and the **Country** that that **Foreign Parent** is from. The *foreign_parent_date* can be stored along the edge linking **Application** to **Foreign Parent**.\n",
    "\n",
    "Let's add this into our overall schema. (sorry, I ran out of unique colors)\n",
    "\n",
    "<img src=\"images/foreign_schema.png\" width=\"700px\" />\n",
    "\n",
    "The keen eyed will have noticed that I added an edge from **Foreign Patent** to our existing **Country** vertex. Even though the **Country** that the **Inventor** is from is a different concept than the **Country** that a **Foreign Parent** corresponds to, a **Country** is still a **Country** and our graph modeling should represent that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pat_term_adj.csv\n",
    "\n",
    "|application_number|pta_sequence_number|pta_event_date|pta_event_desc|applicant_delay_duration|\n",
    "|---|---|---|---|---|\n",
    "|09068213||2002-12-02|Mail Notice of Allowance|0|\n",
    "\n",
    "<br>\n",
    "\n",
    "|uspto_delay_duration|start_pta_sequence_number|term_extension_indicator|\n",
    "|---|---|---|\n",
    "|0||1|\n",
    "\n",
    "As described in [Appendix E](https://www.uspto.gov/sites/default/files/documents/Appendix%20E.pdf), term adjustments account for delays in the patent process. We can further break these down like so.\n",
    "\n",
    "-**PTA Event**\n",
    "- primary id (will need to generate)\n",
    "- pta_event_desc\n",
    "- applicant_delay_duration\n",
    "- uspto_delay_duration\n",
    "\n",
    "-**Extension Indicator**\n",
    "\n",
    "<img src=\"images/pta_schema.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main things to talk about here. The first will be pretty apparent, there's an edge pointing from **PTA Event** back to **PTA Event**. How's that work?\n",
    "\n",
    "Remember that this is our schema, and that these Nodes don't represent actual Vertices, but rather types of Vertices and the relationships that they CAN have to each other. \n",
    "\n",
    "You CAN NOT have an edge that points from **PTA Event 1** back to **PTA Event 1**. Remember that edges are pointers between memory locations and that we need a unique ID for both the vertex at the source and target of the edge.\n",
    "\n",
    "But you CAN have an edge that points from **PTA Event 1** to **PTA Event 2**. Even though **PTA Event 1** and **PTA Event 2** are both of type **PTA Event**, they each have a unique ID and are therefor capable of having an edge connect them.\n",
    "\n",
    "Within the context of our dataset, the `has_start` edge references *start_pta_sequence_number* which ties a particular **PTA Event** to the **PTA Event** whose deadline was missed causing the delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing that we need to talk about with this file is the *Primary ID* for **PTA Event**. We don't have one in the dataset, and we need one to make each **PTA Event** unique within the graph.\n",
    "\n",
    "You might look at this and think: \"Why can't we just use *pta_event_description* as a unique identifier?\" we've used a similar method for **Application Status**, so why won't it work here? The thing stopping us from doing this are the attributes of **PTA Event**. If we knew that *applicant_delay_duration* and *uspto_delay_duration* were the same for each individual instance of *pta_event_description*, then we could do this. But because those delays vary from **Application** to **Application** and are NOT fixed to a given *pta_event_description*, then we need to maintain a unique copy of each **PTA Event**.\n",
    "\n",
    "There are two ways to do this and each has scenarios where it works better than the alternative.\n",
    "\n",
    "#### Method 1 - Unique by Combination\n",
    "\n",
    "Something we haven't talked about yet are called token functions. Token functions are functions that run over our data while it's being loaded into the graph. These functions allow us to do things like convert Epoch time into Datetime, change the case of a string, and many other helpful data manipulation tasks that we would normally have to do before bringing our data into the database.\n",
    "\n",
    "The token function that we'll be talking about for Method 1 is `gsql_concat`. As the name implies, this allows you to concatenate multiple fields of your data into one long string.\n",
    "\n",
    "We can use this to take multiple non-unique fields from our data and combine them into a unique field. For example, we could use a concatenation of *application_number* and *pta_event_date* to create a unique identifier for **PTA Event**. Or at least we could if it was impossible to have multiple **PTA Events** for the same **Application** on the same day.\n",
    "\n",
    "If we start looking through our data, we can see that this is not the case and there can be multiple **PTA Events** on the same *date* for a given **Application**. Fair enough, let's add in more information to ensure our ID is unique. The *pta_event_description* seems to NOT occur multiple times on the same *date* for the same **Application**. We could have our Primary ID be a concatenation of *application_number*, *pta_event_date*, and *pta_event_description* to result in a truly unique ID for this dataset.\n",
    "\n",
    "For the first line of our data file, that would give us the Primary ID of: `090682132002-12-02Mail Notice of Allowance`. What a mouthful! \n",
    "\n",
    "This works and will function fine in our graph. However, it's not very pretty. This method usually works best when you only need to concatenate 2 fields and the resulting concatenation will be something useful for a user to identify the resulting vertex.\n",
    "\n",
    "Say you had a dataset for playing cards and the only two columns of data you had were the **Symbol** on the card and the **Value** on the card. There's 13 different **Values** for each **Symbol**, so we can't use that as a Primary ID. And there's 4 different **Symbols** for each **Value**, so that's not unique either. However, there is only one **Card** per combination of **Symbol** and **Value**. Because of this, a concatenation of **Symbol** and **Value** will provide both a Unique ID and a human readable value that can easily describe a given **Card**. A sample Primary ID would be something like `Hearts3` or `SpadesQ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 - Unique by Design\n",
    "\n",
    "We don't always need a human readable Primary ID for our data, and in the case of **PTA Event** we have plenty of attributes to help us understand the **PTA Event** so the ID doesn't need to be too descriptive. You're saying, \"If we don't need it to be human readable, then why didn't we just use the `gsql_concat` token function mentioned above?\" The answer is because there's an easier way.\n",
    "\n",
    "`gsql_uuid_v4` will generate a Unique ID. That's all it does, spits out an ID that is unique to all other vertices in the graph. It outputs a long string of numbers and letters resembling this: `4493d5ce-a69b-4c90-88e4-b41e9f576169`, it's not pretty, but it's guaranteed unique.\n",
    "\n",
    "This method works for scenarios like this where `concat` might not guarantee a Unique ID across a huge dataset and you don't necessarily care about the value of the Primary ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pta_summary.csv\n",
    "\n",
    "|application_number|pto_delay_a|pto_delay_b|pto_delay_c|overlap_pto_delay|\n",
    "|---|---|---|---|---|\n",
    "|09743549|0|0|0|0|\n",
    "\n",
    "<br>\n",
    "\n",
    "|nonoverlap_pto_delay|pto_manual_adjustment|applicant_delay|patent_term_adjustment\n",
    "|---|---|---|---|\n",
    "|0|0|0|0|\n",
    "\n",
    "-**PTA Summary**\n",
    "- primary id (need to generate)\n",
    "- pto_delay_a\n",
    "- pto_delay_b\n",
    "- pto_delay_c\n",
    "- overlay_pto_delay\n",
    "- nonoverlap_pto_delay\n",
    "- pto_manual_adjustment\n",
    "- applicant_delay\n",
    "- patent_term_adjustment\n",
    "\n",
    "**PTA Summary** builds off of **PTA Event** and provides us with the total delay time of multiple types for the duration of the **Application**. Where **PTA Event** shows the delay caused by each **PTA Event**, **PTA Summary** shows the summation of all delays for a given **Application**. \n",
    "\n",
    "Technically, we don't need this. We could gather the same information by selecting every **PTA Event** tied to an **Application** and summing the respective delay attributes. That however requires traversals equal to the number of **PTA Events** for an **Application** AND we need to access attributes of each **PTA Event** that we hit, which will cost performance. Conversely, we only need to access ONE **PTA Summary** to gather that same information.\n",
    "\n",
    "You can learn what each delay type represents in [Appendix E](https://www.uspto.gov/sites/default/files/documents/Appendix%20E.pdf) of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pte_summary.csv\n",
    "\n",
    "|application_number|pto_adjustment|pto_delay|applicant_delay|patent_term_extension|\n",
    "|---|---|---|---|---|\n",
    "|09068213|0|0|0|0|\n",
    "\n",
    "-**PTE Summary**\n",
    "- primary ID (need to generate)\n",
    "- pto_adjustment\n",
    "- pto_delay\n",
    "- applicant_delay\n",
    "- patent_term_extension\n",
    "\n",
    "This file isn't mentioned in our Appendix for some reason, so we have to infer some info from it. First, we can guess that PTE stands for Patent Term Extension and due to the similarity of this file to `pta_summary.csv` we can assume that this file describes the cumulative extensions to a given **Application**.\n",
    "\n",
    "We've added quite a few vertices and edges to our schema, let's see what the whole thing looks like so far.\n",
    "\n",
    "<img src=\"images/pte_schema.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continuity_children.csv and continuity_parents.csv\n",
    "\n",
    "`continuity_children.csv`\n",
    "|application_number|child_application_number|child_filing_date|\n",
    "|---|---|---|\n",
    "|02262037|59997901|2018-01-01|\n",
    "\n",
    "<br>\n",
    "\n",
    "`continuity_parents.csv`\n",
    "|application_number|parent_application_number|parent_filing_date|continuation_type|\n",
    "|---|---|---|---|\n",
    "|05354590|05101449|1970-12-28|CON|\n",
    "\n",
    "\n",
    "-**Continuation Type**\n",
    "\n",
    "-**`has_child`**\n",
    "- filing_date\n",
    "\n",
    "-**`has_parent`**\n",
    "- filing_date\n",
    "\n",
    "-**`is_continuation_type`**\n",
    "\n",
    "[Appendix C](https://www.uspto.gov/sites/default/files/documents/Appendix%20C.pdf) for this one. Essentially, continuing a patent allows you to file child applications that can add to, augment, reissue, and more to your initial **Patent**. I'm no patent expert, so you can read up more about the different *continuation_types* somewhere like [wikipedia](https://en.wikipedia.org/wiki/Continuing_patent_application) or [Appendix C](https://www.uspto.gov/sites/default/files/documents/Appendix%20C.pdf).\n",
    "\n",
    "<img src=\"images/continuation_schema.png\" width=\"400px\" />\n",
    "\n",
    "The `has_parent` - `has_child` relationship is the epitome of a directed edge use case. **Continuation Type** is broken out so that we can use it as a filter, but you knew that by now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correspondence_address.csv\n",
    "\n",
    "|application_number|correspondence_name|correspondence_street_line_1|correspondence_street_line_2|\n",
    "|---|---|---|---|\n",
    "|04526546|WILLIAMS D. HALL|200 SEMMES BUILDING|10220 RIVER ROAD|\n",
    "\n",
    "<br>\n",
    "\n",
    "|correspondence_street_line_3|correspondence_city|correspondence_postal_code|correspondence_region_code|\n",
    "|---|---|---|---|\n",
    "||POTOMAC|20854|MD|\n",
    "\n",
    "<br>\n",
    "\n",
    "|correspondence_country_code|customer_number|\n",
    "|---|---|\n",
    "|US||\n",
    "\n",
    "-**Correspondence**\n",
    "- name\n",
    "- customer_number\n",
    "\n",
    "-**Address**\n",
    "- street_line_1\n",
    "- street_line_2\n",
    "- street_line_3\n",
    "- city\n",
    "- postal_code\n",
    "- region\n",
    "- country\n",
    "\n",
    "-**Postal Code**\n",
    "\n",
    "[Appendix F](https://www.uspto.gov/sites/default/files/documents/Appendix%20F.pdf)\n",
    "\n",
    "This might not seem like the most valuable information, but we can actually do something cool here and use this data alongside our existing **City**, **Country**, and **Region** data.\n",
    "\n",
    "Before:\n",
    "\n",
    "<img src=\"images/location_1.png\" width=\"500\" />\n",
    "\n",
    "With `correspondence_address.csv`:\n",
    "\n",
    "<img src=\"images/location_2.png\" width=\"500\" />\n",
    "\n",
    "Fully Connected:\n",
    "\n",
    "<img src=\"images/location_3.png\" width=\"500\" />\n",
    "\n",
    "Okay, that's a little messy, but essentially we're able to connect **Address** and **Postal Code** to their corresponding **City**, **Region**, and **Country** vertices. The richer the connections in our graph, the more information we can infer about the relationships between different entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all Together\n",
    "\n",
    "This is it! The moment we've been looking forward to, maybe even fearing this whole time. What does the complete schema look like?\n",
    "\n",
    "<img src=\"images/full_schema.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our Schema in TigerGraph\n",
    "\n",
    "Now that we have a good idea what our schema will look like, it's time to get it set up in TigerGraph. Once that's done, we'll map our data file and finally load our data.\n",
    "\n",
    "We're going to look at two different ways of defining our schema, in the GraphStudio UI, and via GSQL code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can get into creating the schema, we need to talk about how schema works in TigerGraph. The main thing to note here is that you can have both a **Global** schema and a **Graph** schema. Because you can have multiple **Graphs** on one TigerGraph solution, those **Graphs** need to be able to maintain a schema specific to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Schema\n",
    "\n",
    "This is an overarching schema that can be used by as many or as few **Graphs** as you desire. The purpose of the **Global Schema** is to give you a common schema that you CAN pull from when setting up **Graphs**. This is helpful if you have common elements between your different **Graphs** and want those common elements to have the same schema. For example, my **Global Schema** might contain the entirety of information about my supply chain. Everything from suppliers, warehouses, transportation lines, user orders, product information, user shipping information. However, one of our **Graphs** might want to only contain information about the suppliers, warehouses, and transportation as that side of the business doesn't need access to customer information. I could use just those elements and their edges from the **Global Schema** to populate this **Graph** without having to include all fo the information about the ordering user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Schema\n",
    "\n",
    "As we talked about above, the **Graph Schema** can contain as much or none of the **Global Schema** as you desire. In addition, the **Graph Schema** can contain elements that are not in the **Global Schema**. From the example above, I want to keep track of the manufacturing side of my supply chain. I can include the suppliers, warehouses, and transportation from the **Global Schema**, but I can also add in something like weather forecasts and supply forecasts that could be used to predict disruptions to my manufacturing chain. This data can exist in the **Graph Schema** for the Manufacturing Graph, but not in the **Global Schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Conventions\n",
    "\n",
    "Let's talk naming conventions really quickly. When building a complicated graph, you will have a variety of Vertex Types, Edge Types, and attributes. I recommend that you use different naming conventions for each so that way you can easily remember what aspect of the graph a variable is based on how it's named. This will come in very handy when writing queries.\n",
    "\n",
    "I'm going to share the naming convention I use, but you should use what's comfortable to you, or what fits any existing work you're trying to integrate into.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Vertex Types\n",
    "\n",
    "Vertex name describes the concept that the vertex represents\n",
    "\n",
    "**PascalCase** - all word beginnings are capital letters\n",
    "- FileLocation\n",
    "- ContinuationType\n",
    "- PGPUBNumber\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Edge Types\n",
    "\n",
    "Edge name includes verb describing relationship and denotation of target vertex type\n",
    "\n",
    "**snake_case** - all words are lowercase and separated by an underscore\n",
    "- has_patent\n",
    "- at_location\n",
    "- from_city\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Attributes\n",
    "\n",
    "Attribute name describes attribute field an can usually just be the column name from the source table (reformatted into your choice naming convention)\n",
    "\n",
    "**camelCase** - first word is lowercase, subsequent words start with a capital letter\n",
    "- filingDate\n",
    "- fullName\n",
    "- description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphStudio Schema Design\n",
    "\n",
    "The GraphStudio tool is extremely helpful when you are first defining your schema. The UI makes it very easy to see exactly what the graph schema you are creating will look like. It also provides easy access to all of the options and customizations you can make to a vertex or edge when defining it. The UI is FANTASTIC when you are initially setting up your schema and still figuring out how everything will fit together.\n",
    "\n",
    "Where the UI lacks is it's repeatability. You don't want to have to spend time clicking around to set up your schema again in the event that you need to re-do it. Luckily that's where GSQL comes in, and even more fortunately, we can export all of the hard work that we do in GraphStudio as GSQL, but we'll get to that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Graph\n",
    "\n",
    "We are going to be working within the **Graph Schema** here. When we are doing our initial data explorations, it is easier for us to adjust the **Graph Schema** than the **Global Schema**. Changing the **Global Schema** will require any Graphs using the changed part of the **Global Schema** to be dropped. This means you will have to reload the entirety of the data and re-do the mapping. When changing the **Graph Schema**, only the vertices or edges of the changed type will have to be dropped. \n",
    "\n",
    "<img src=\"images/create_graph_1.png\" width=\"500px\" /> <img src=\"images/create_graph_2.png\" width=\"250px\" /> <img src=\"images/create_graph_3.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Vertex\n",
    "\n",
    "On the **Design Schema** tab, use the **White Plus icon** to define a vertex type.\n",
    "\n",
    "<img src=\"images/create_vertex_1.png\" width=\"500px\" />\n",
    "\n",
    "This will bring up a panel allowing us to further describe the **Vertex Type**. I'll walk through each of the fields with a vertex from our dataset.\n",
    "\n",
    "<img src=\"images/create_vertex_2.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Application Vertex\n",
    "\n",
    "As a refresher we'll use the **Application** vertex here. Which looks like this:\n",
    "\n",
    "**Application**\n",
    "|Attribute|Type|\n",
    "|---|---|\n",
    "|applicationNumber (primary_id)|string|\n",
    "|filingDate|datetime|\n",
    "|confirmationNumber|string|\n",
    "|docketNumber|string|\n",
    "|title|string|\n",
    "\n",
    "I was able to extract the **Type** information from the Appendix of our data.\n",
    "\n",
    "<img src=\"images/application_data_schema.png\" width=\"500px\" />\n",
    "\n",
    "You'll notice that I used **DateTime** even though *filingDate* specified a **float**. In the original database, that *filingDate* is stored as epoch time as a float, but formatted as a date as we can see from the `%td` formatting specifier. Looking through the actual data files will show that the *filingDate* field is formatted as `year-month-day` (1968-11-18) and that doesn't look like any float I've ever seen but it certainly looks like a DateTime. Under the hood, TigerGraph stores DateTime as an **int**, however using **dateTime** allows us to use **dateTime** functions in our queries without first having to convert from **int**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vertex Type and ID\n",
    "\n",
    "Of the fields on the *Create Vertex* tab, only two of them are REQUIRED. Those are the **Vertex type name** and **Primary id**. The **Vertex type name** is the name that we'll use to refer to that vertex type by. **Primary id** is where we set the name of the field that will contain the Primary ID of the vertex. The variable type of the Primary ID will also need to be set to the relevant value for the data type being loaded in.\n",
    "\n",
    "<img src=\"images/create_vertex_3.png\" width=\"500px\" />\n",
    "\n",
    "Additionally, you will find a checkbox for **As attribute**. Normally, the **Primary id** of a vertex is not accessible when querying the vertex. If we would like to be able to access the **Primary id** like we would an attribute, then we will need to check the **As attribute** box. For example, with our **Application** vertex type here, we'll be using the *applicationNumber* as the **Primary id**. WITHOUT it set as an attribute, we could return **Application** from a query and read all of it's attributes, but we wouldn't be able to see the *applicationNumber*. Setting **As attribute** will allow us to return that *applicationNumber* as well. As an example of when this would not be necessary, recall the **PTAEvent** vertex type from earlier. We had to generate a unique id for that vertex because one did not exist in our source dataset. This ID isn't any important other than providing a unique identifier to the graph engine (if it was we wouldn't have to generate it). In this scenario, we wouldn't need to select **As attribute** because we don't care what that unique ID is, we just care about the attribute information contained in the rest of the vertex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes\n",
    "\n",
    "<img src=\"images/create_vertex_4.png\" width=\"500px\" />\n",
    "\n",
    "Attributes contain the actual informational contents of a **Vertex**. These are the values that will be available to you when querying a vertex. \n",
    "\n",
    "- **Attribute name** is pretty self explanatory, this is the name that will be used to reference the attribute when querying and loading data.\n",
    "- **Attribute type** is the variable type of the attribute. You can review the list of available types [here](https://docs.tigergraph.com/gsql-ref/current/querying/data-types).\n",
    "- **Default value** this is an override for the default value that an attribute will assume if no data is provided. You do not need ot set this unless you are looking to override the default value defined [here](https://docs.tigergraph.com/gsql-ref/current/querying/data-types#_base_types)\n",
    "- **Index** this checkbox will denote that this attribute is intended to be used as a secondary index for the **Vertex type**. You can only have one attribute be an **Index** per **Vertex type**.\n",
    "\n",
    "<img src=\"images/create_vertex_5.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style\n",
    "\n",
    "This last section only applies to the GraphStudio interface. Here you can select the **Color** and optional **Icon** that can represent a vertex type.\n",
    "\n",
    "<img src=\"images/create_vertex_6.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Edge\n",
    "\n",
    "Select the **Arrow** icon from the toolbar at the top of the **Design Schema** page. The **Arrow** will turn blue indicating that it is active.\n",
    "\n",
    "<img src=\"images/create_edge_1.png\" width=\"500px\" />\n",
    "\n",
    "Select the *Source* vertex, then the *Target* vertex. If your edge is un-directed the selection order does not matter.\n",
    "\n",
    "<img src=\"images/create_edge_2.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge type name\n",
    "\n",
    "This is the name that we will be using to reference the **Edge type**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directed\n",
    "\n",
    "This is where we can select wether or not an edge is **Directed**. When selecting directed edge, the **Reverse Edge** checkbox and **Reverse edge type** field will appear. \n",
    "\n",
    "<img src=\"images/create_edge_3.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source and target vertex types\n",
    "\n",
    "Here we declare the **Source** and **Target** vertex types. Edges can either have a singular **Source** - **Target** pair, or can exist as **Compound Edges** with multiple **Source** - **Target** pairs. \n",
    "\n",
    "<img src=\"images/create_edge_4.png\" width=\"500px\" />\n",
    "\n",
    "The **Source** and **Target** can both be the same **Vertex Type**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes\n",
    "\n",
    "The attributes selection for our edges is the same as the attributes section for the vertices.\n",
    "\n",
    "<img src=\"images/create_edge_5.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publish Schema\n",
    "\n",
    "<img src=\"images/create_edge_6.png\" width=\"500px\" />\n",
    "\n",
    "Currently the schema only exists within the GraphStudio interface. Use the **Publish Schema** button to save the schema to the TigerGraph server. You will be asked to confirm the schema changes, then the schema will be published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSQL Schema Design\n",
    "\n",
    "For times when you don't want to click through the interface in order to create your schema, you can do it via TigerGraph's language **GSQL**. **GSQL** can be executed on your TigerGraph server via the **GSQL** terminal or remotely by one of our many TigerGraph connectors. For this example we'll be using [pyTigerGraph](https://github.com/pyTigerGraph/pyTigerGraph) to interface through this Python notebook. The GSQL will remain the same regardless of which connector method you use.\n",
    "\n",
    "We're going to walk through the same vertex and edge creation that we went through with **Application** and **is_continuation_type** except this time in **GSQL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Vertex\n",
    "\n",
    "This is what the GSQL required to create the **Application** vertex looks like:\n",
    "\n",
    "`CREATE VERTEX Application(PRIMARYID id STRING, filingDate DATETIME, confirmationNumber STRING, docketNumber STRING, title STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"`\n",
    "\n",
    "To simplify things, this is the pattern your most basic vertex deceleration will follow:\n",
    "\n",
    "`CREATE VERTEX <VertexType>(PRIMARYID id <DataType>, <attributeName1> <DataType1>) WITH PRIMARYIDASATTRIBUTE=\"true\"`\n",
    "\n",
    "Additional attributes are separated by commas and placed after the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Edge\n",
    "\n",
    "The GSQL to create an edge is extremely similar to that to create a vertex. Here's the **is_continuation_type** edge:\n",
    "\n",
    "`CREATE UNDIRECTED EDGE is_continuation_type(FROM Application, TO ContinuationIype)`\n",
    "\n",
    "Let's look at a **Directed** edge for comparison:\n",
    "\n",
    "`CREATE DIRECTED EDGE has_child(FROM Application, TO Application, date DATETIME) WITH REVERSEIDGE=\"reverse_has_child\"`\n",
    "\n",
    "And lastly, the pattern ( [  ] = **Optional** ):\n",
    "\n",
    "`CREATE DIRECTED|UNDIRECTED EDGE <edge_name>(FROM <VertexType> TO <VertexType>, <attributeName1> <DataType1>) [WITH REVERSEADGE=<reverse_edge_name>]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compound Edges\n",
    "\n",
    "This is a slightly more advanced concept, but it's going to be useful in our example. We're actually going to modify our schema slightly to simplify edge names. \n",
    "\n",
    "This was the previous schema for the location section of our graph:\n",
    "\n",
    "<img src=\"images/location_3.png\" width=\"500\" />\n",
    "\n",
    "With the ability to use the same **Edge Name** for multiple sets of Source - Target connection, we can make our schema much simpler. Edges of the same color share the same **Edge Name**.\n",
    "\n",
    "<img src=\"images/location_4.png\" width=\"500\" />\n",
    "\n",
    "It's important to note here that we are only doing this when the edges are expressing the same relationship. For example:\n",
    "\n",
    "<img src=\"images/compound_1.png\" width=\"300\" />\n",
    "\n",
    "**Application** is linked to both **Type** and **ContinuationType**. Both of these vertices represent a type so why doesn't it make sense to call both edges **has_type**? It's because these are different concepts. **Type** is not the same as **ContinuationType**. If there was another concept in our graph that related back to our **Type** variable, then we could call that connection **has_type** because it is referencing the same **Target vertex** of the other **has_type** edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what a **Compound Edge** looks like.\n",
    "\n",
    "`CREATE UNDIRECTED EDGE in_code (FROM Address, TO PostalCode | FROM Correspondence, TO PostalCode | FROM City TO, PostalCode)`\n",
    "\n",
    "Notice that pipes separate the individual **Source** - **Target** pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Schema\n",
    "\n",
    "Now it's time to declare our whole schema. I'm going to separate the declarations by vertices and edges, but there is nothing stopping you from declaring your whole schema in one statement.\n",
    "\n",
    "Read through this and see how it lines up with the visual schema that we created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertices\n",
    "\n",
    "```\n",
    "CREATE VERTEX Application(PRIMARYID id STRING, filingDate DATETIME, confirmationNumber STRING, docketNumber STRING, title STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ContinuationType(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX USPCClass(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX USPCSubclass(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX EventCode(PRIMARYID id STRING, description STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PTAEvent(PRIMARYID id STRING, description STRING, applicantDelay FLOAT, usptoDelay FLOAT) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ExtensionIndicator(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PTASummary(PRIMARYID id STRING, ptoDelayA FLOAT, ptoDelayB FLOAT, ptoDelayC FLOAT, overlapDelay FLOAT, nonOverlapDelay FLOAT, manualAdjustment FLOAT, applicationDelay FLOAT, PTA FLOAT) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PTESummay(PRIMARYID id STRING, ptoAdjustment FLOAT, ptoDelay FLOAT, applicantDelay FLOAT, PTE FLOAT) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ApplicationStatus(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PatentNumber(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Examiner(PRIMARYID id STRING, fullName STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ArtUnit(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Attorney(PRIMARYID id STRING, firstName STRING, middleName STRING, lastName STRING, suffix STRING, phone STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PracticeCategory(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX SmallEntity(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX First_toFile(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX FileLocation(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PGPUBNumber(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX WIPONumber(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Inventor(PRIMARYID id STRING, first STRING, middle STRING, last STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ForeignParent(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX City(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Region(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Country(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PostalCode(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Correspondence(PRIMARYID id STRING, name STRING, customerNumber STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Address(PRIMARYID id STRING, line1 STRING, line2 STRING, line3 STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges\n",
    "\n",
    "```\n",
    "CREATE DIRECTED EDGE has_child(FROM Application, TO Application, date DATETIME) WITH REVERSEIDGE=\"reverse_has_child\"\n",
    "CREATE DIRECTED EDGE has_parent(FROM Application, TO Application, date DATETIME) WITH REVERSEIDGE=\"reverse_has_parent\"\n",
    "CREATE UNDIRECTED EDGE is_continuation_type(FROM Application, TO ContinuationType)\n",
    "CREATE UNDIRECTED EDGE has_class(FROM Application, TO USPCClass)\n",
    "CREATE UNDIRECTED EDGE has_subclass(FROM Application, TO USPCSubclass)\n",
    "CREATE DIRECTED EDGE is_subclass(FROM USPCSubclass, TO USPCClass) WITH REVERSEADGE=\"reverse_is_subclass\"\n",
    "CREATE UNDIRECTED EDGE has_code(FROM Application, TO EventCode, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE has_pta_event(FROM Application, TO PTAEvent, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE is_extension(FROM PTAEvent, TO ExtensionIndicator)\n",
    "CREATE DIRECTED EDGE has_start(FROM PTAEvent, TO PTAEvent) WITH REVERSEADGE=\"reverse_has_start\"\n",
    "CREATE UNDIRECTED EDGE has_pta_summary(FROM Application, TO PTASummary)\n",
    "CREATE UNDIRECTED EDGE has_pte_summary(FROM Application, TO PTESummay)\n",
    "CREATE UNDIRECTED EDGE has_status(FROM Application, TO ApplicationStatus, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE has_patent(FROM Application, TO PatentNumber, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE has_examiner(FROM Application, TO Examiner)\n",
    "CREATE UNDIRECTED EDGE from_unit(FROM Examiner, TO ArtUnit)\n",
    "CREATE UNDIRECTED EDGE has_attorney(FROM Application, TO Attorney)\n",
    "CREATE UNDIRECTED EDGE has_practice_category(FROM Attorney, TO PracticeCategory)\n",
    "CREATE UNDIRECTED EDGE is_small(FROM Application, TO SmallEntity)\n",
    "CREATE UNDIRECTED EDGE follows_ftf(FROM Application, TO First_toFile)\n",
    "CREATE UNDIRECTED EDGE at_location(FROM Application, TO FileLocation, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE hasIGPUB(FROM Application, TO PGPUBNumber, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE hasIIPO(FROM Application, TO WIPONumber, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE filed_application(FROM Application, TO Inventor, rank INT)\n",
    "CREATE UNDIRECTED EDGE has_foreign_parent(FROM Application, TO ForeignParent, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE from_city(FROM Inventor, TO City)\n",
    "CREATE UNDIRECTED EDGE from_region(FROM Inventor, TO Region)\n",
    "CREATE UNDIRECTED EDGE from_country(FROM Inventor, TO Country)\n",
    "CREATE UNDIRECTED EDGE filed_country(FROM ForeignParent, TO Country)\n",
    "CREATE UNDIRECTED EDGE has_correspondence(FROM Inventor, TO Correspondence)\n",
    "CREATE UNDIRECTED EDGE has_address(FROM Address, TO Correspondence)\n",
    "CREATE UNDIRECTED EDGE in_code(FROM City, TO PostalCode | FROM Correspondence, TO PostalCode | FROM Address, TO PostalCode)\n",
    "CREATE UNDIRECTED EDGE in_region(FROM PostalCode, TO Region | FROM Correspondence, TO Region | FROM Address, TO Region)\n",
    "CREATE UNDIRECTED EDGE in_country(FROM PostalCode, TO Country | FROM Country, TO Region | FROM Correspondence, TO Country | FROM Address, TO Country)\n",
    "CREATE UNDIRECTED EDGE in_city(FROM Region, TO City | FROM Correspondence, TO City | FROM Address, TO City)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing the Schema with GSQL\n",
    "\n",
    "We'll use [pyTigerGraph](https://github.com/pyTigerGraph/pyTigerGraph) for this, but you can do this though any GSQL interface on your TigerGraph server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyTigerGraph -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the TigerGraph server\n",
    "\n",
    "I'm not going to go to into detail here as this login process is thoroughly documented in our [pyTigerGraph 101](https://colab.research.google.com/drive/1fJpcv-q0NLfHj3X1k6Lbwddp8gVVcfES) learning.\n",
    "\n",
    "The long and skinny of it is that we need to first connect to our TigerGraph server, then we need to securely authenticate with the **Graph** itself. Because the **Graph** can contain actual data (**global** space is just schema, no data) a more secure connection is needed via a token. These tokens can also be used to manage access roles for different users, but you can read about that [here](https://docs.tigergraph.com/tigergraph-server/3.3/user-access/roles-and-privileges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyTigerGraph as tg\n",
    "\n",
    "# connection parameters\n",
    "# hostName is the TigerGraph solution URL\n",
    "hostName = \"https://patent-free.i.tgcloud.io\"\n",
    "graphName = \"patents\"\n",
    "userName = \"tigergraph\"\n",
    "password = \"tigergraph\"\n",
    "\n",
    "# establish the connection to the TigerGraph Solution\n",
    "conn = tg.TigerGraphConnection(host=hostName, username=userName, password=password)\n",
    "\n",
    "# print any current schema so we can verify that we are connected\n",
    "conn.gsql('LS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Connection\n",
    "\n",
    "Once connected to the Solution, we can create the required Secret and Token needed to authenticate with our Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Application': 0, 'Continuation': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the name of the graph that we want to connect to\n",
    "conn.graphname = graphName\n",
    "\n",
    "# create a secret\n",
    "secret = conn.createSecret()\n",
    "# use the secret to get a token\n",
    "authToken = conn.getToken(secret)[0]\n",
    "\n",
    "# connect to graph with token\n",
    "conn = tg.TigerGraphConnection(host=hostName, username=userName, password=password, graphname=graphName, apiToken=authToken)\n",
    "\n",
    "# listing vertex count requires graph authentication and will prove that we're securely connected to the Graph\n",
    "conn.getVertexCount(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearing the Schema (optional)\n",
    "\n",
    "Since we're going to be loading in the full schema, we should clear any currently loaded schema from the graph as it will throw an error if we try to create a Vertex or Edge with the same name as one that already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of vertices and edges currently in the graph schema\n",
    "vertices = conn.getVertexTypes()\n",
    "edges = conn.getEdgeTypes()\n",
    "\n",
    "# we need a SCHEMAIHANGE JOB to change the schema, we're going to put that together in the next couple lines\n",
    "changeJob = '''CREATE SCHEMAIHANGE JOB clearGraph FOR GRAPH patents {'''\n",
    "\n",
    "for vertex in vertices:\n",
    "    changeJob += ('''DROP VERTEX ''' + vertex + ';')\n",
    "\n",
    "for edge in edges:\n",
    "    changeJob += ('''DROP EDGE ''' + edge + ';')\n",
    "\n",
    "changeJob += '}'\n",
    "\n",
    "# print the complete change job\n",
    "print(changeJob)\n",
    "\n",
    "# add the job to the graph\n",
    "print(conn.gsql('''\n",
    "    USE GRAPH patents\n",
    "    ''' + changeJob))\n",
    "# execute the change job\n",
    "print(conn.gsql('''\n",
    "    USE GRAPH patents\n",
    "    RUN SCHEMAAHANGE JOB clearGraph\n",
    "    '''))\n",
    "# delete the change job\n",
    "print(conn.gsql('''\n",
    "    USE GRAPH patents\n",
    "    DROP JOB clearGraph\n",
    "    '''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearing the Graph\n",
    "\n",
    "Via GSQL, it's much easier for us to create our schema in the **Global** manner, then create a **Graph** utilizing elements from that **Global** schema. Adding schema to an existing **Graph** is possible, it just requites a [SchemaIhange Job](https://docs.tigergraph.com/gsql-ref/3.3/ddl-and-loading/modifying-a-graph-schema) rather than schema definition.\n",
    "\n",
    "Because this process is creating a new **Graph** from our **Global** schema, we need to drop our old **Graph** because we can't have two graphs with the same name.\n",
    "\n",
    "Remember how we needed a **Secret** and **Token** to connect to a graph specifically? Once we delete our current Graph and create a new one, we will need to create a new **Secret** and **Token** tied to that Graph so we can connect to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph Patents is dropped.\n"
     ]
    }
   ],
   "source": [
    "print(conn.gsql('DROP GRAPH patents'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Schema\n",
    "\n",
    "It's time to load in our schema. I'm going to do this in separate parts for vertices and edges, but both could be done in the same statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Successfully created vertex types: [Application].\\nSuccessfully created vertex types: [ContinuationType].\\nSuccessfully created vertex types: [USPCClass].\\nSuccessfully created vertex types: [USPCSubclass].\\nSuccessfully created vertex types: [EventCode].\\nSuccessfully created vertex types: [PTAEvent].\\nSuccessfully created vertex types: [ExtensionIndicator].\\nSuccessfully created vertex types: [PTASummary].\\nSuccessfully created vertex types: [PTESummay].\\nSuccessfully created vertex types: [ApplicationStatus].\\nSuccessfully created vertex types: [PatentNumber].\\nSuccessfully created vertex types: [Examiner].\\nSuccessfully created vertex types: [ArtUnit].\\nSuccessfully created vertex types: [Attorney].\\nSuccessfully created vertex types: [PracticeCategory].\\nSuccessfully created vertex types: [SmallEntity].\\nSuccessfully created vertex types: [First_toFile].\\nSuccessfully created vertex types: [FileLocation].\\nSuccessfully created vertex types: [PGPUBNumber].\\nSuccessfully created vertex types: [WIPONumber].\\nSuccessfully created vertex types: [Inventor].\\nSuccessfully created vertex types: [ForeignParent].\\nSuccessfully created vertex types: [City].\\nSuccessfully created vertex types: [Region].\\nSuccessfully created vertex types: [Country].\\nSuccessfully created vertex types: [PostalCode].\\nSuccessfully created vertex types: [Correspondence].\\nSuccessfully created vertex types: [Address].'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vertices\n",
    "conn.gsql('''\n",
    "CREATE VERTEX Application(PRIMARYID id STRING, filingDate DATETIME, confirmationNumber STRING, docketNumber STRING, title STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ContinuationType(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX USPCClass(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX USPCSubclass(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX EventCode(PRIMARYID id STRING, description STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PTAEvent(PRIMARYID id STRING, description STRING, applicantDelay FLOAT, usptoDelay FLOAT) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ExtensionIndicator(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PTASummary(PRIMARYID id STRING, ptoDelayA FLOAT, ptoDelayB FLOAT, ptoDelayC FLOAT, overlapDelay FLOAT, nonOverlapDelay FLOAT, manualAdjustment FLOAT, applicationDelay FLOAT, PTA FLOAT) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PTESummay(PRIMARYID id STRING, ptoAdjustment FLOAT, ptoDelay FLOAT, applicantDelay FLOAT, PTE FLOAT) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ApplicationStatus(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PatentNumber(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Examiner(PRIMARYID id STRING, fullName STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ArtUnit(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Attorney(PRIMARYID id STRING, firstName STRING, middleName STRING, lastName STRING, suffix STRING, phone STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PracticeCategory(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX SmallEntity(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX First_toFile(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX FileLocation(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PGPUBNumber(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX WIPONumber(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Inventor(PRIMARYID id STRING, first STRING, middle STRING, last STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX ForeignParent(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX City(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Region(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Country(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX PostalCode(PRIMARYID id STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Correspondence(PRIMARYID id STRING, name STRING, customerNumber STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "CREATE VERTEX Address(PRIMARYID id STRING, line1 STRING, line2 STRING, line3 STRING) WITH PRIMARYIDASATTRIBUTE=\"true\"\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges\n",
    "conn.gsql('''\n",
    "CREATE DIRECTED EDGE has_child(FROM Application, TO Application, date DATETIME) WITH REVERSEIDGE=\"reverse_has_child\"\n",
    "CREATE DIRECTED EDGE has_parent(FROM Application, TO Application, date DATETIME) WITH REVERSEIDGE=\"reverse_has_parent\"\n",
    "CREATE UNDIRECTED EDGE is_continuation_type(FROM Application, TO ContinuationType)\n",
    "CREATE UNDIRECTED EDGE has_class(FROM Application, TO USPCClass)\n",
    "CREATE UNDIRECTED EDGE has_subclass(FROM Application, TO USPCSubclass)\n",
    "CREATE DIRECTED EDGE is_subclass(FROM USPCSubclass, TO USPCClass) WITH REVERSEADGE=\"reverse_is_subclass\"\n",
    "CREATE UNDIRECTED EDGE has_code(FROM Application, TO EventCode, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE has_pta_event(FROM Application, TO PTAEvent, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE is_extension(FROM PTAEvent, TO ExtensionIndicator)\n",
    "CREATE DIRECTED EDGE has_start(FROM PTAEvent, TO PTAEvent) WITH REVERSEADGE=\"reverse_has_start\"\n",
    "CREATE UNDIRECTED EDGE has_pta_summary(FROM Application, TO PTASummary)\n",
    "CREATE UNDIRECTED EDGE has_pte_summary(FROM Application, TO PTESummay)\n",
    "CREATE UNDIRECTED EDGE has_status(FROM Application, TO ApplicationStatus, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE has_patent(FROM Application, TO PatentNumber, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE has_examiner(FROM Application, TO Examiner)\n",
    "CREATE UNDIRECTED EDGE from_unit(FROM Examiner, TO ArtUnit)\n",
    "CREATE UNDIRECTED EDGE has_attorney(FROM Application, TO Attorney)\n",
    "CREATE UNDIRECTED EDGE has_practice_category(FROM Attorney, TO PracticeCategory)\n",
    "CREATE UNDIRECTED EDGE is_small(FROM Application, TO SmallEntity)\n",
    "CREATE UNDIRECTED EDGE follows_ftf(FROM Application, TO First_toFile)\n",
    "CREATE UNDIRECTED EDGE at_location(FROM Application, TO FileLocation, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE hasIGPUB(FROM Application, TO PGPUBNumber, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE hasIIPO(FROM Application, TO WIPONumber, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE filed_application(FROM Application, TO Inventor, rank INT)\n",
    "CREATE UNDIRECTED EDGE has_foreign_parent(FROM Application, TO ForeignParent, date DATETIME)\n",
    "CREATE UNDIRECTED EDGE from_city(FROM Inventor, TO City)\n",
    "CREATE UNDIRECTED EDGE from_region(FROM Inventor, TO Region)\n",
    "CREATE UNDIRECTED EDGE from_country(FROM Inventor, TO Country)\n",
    "CREATE UNDIRECTED EDGE filed_country(FROM ForeignParent, TO Country)\n",
    "CREATE UNDIRECTED EDGE has_correspondence(FROM Inventor, TO Correspondence)\n",
    "CREATE UNDIRECTED EDGE has_address(FROM Address, TO Correspondence)\n",
    "CREATE UNDIRECTED EDGE in_code(FROM City, TO PostalCode | FROM Correspondence, TO PostalCode | FROM Address, TO PostalCode)\n",
    "CREATE UNDIRECTED EDGE in_region(FROM PostalCode, TO Region | FROM Correspondence, TO Region | FROM Address, TO Region)\n",
    "CREATE UNDIRECTED EDGE in_country(FROM PostalCode, TO Country | FROM Country, TO Region | FROM Correspondence, TO Country | FROM Address, TO Country)\n",
    "CREATE UNDIRECTED EDGE in_city(FROM Region, TO City | FROM Correspondence, TO City | FROM Address, TO City)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Graph\n",
    "\n",
    "Here we will declare which vertices and edges that we will include in the Graph that we're setting up.\n",
    "\n",
    "This will take **~2 minutes** to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The graph Patents is created.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.gsql(('CREATE GRAPH Patents(Application, ContinuationType, USPCClass, USPCSubclass, EventCode, PTAEvent, '\n",
    "'ExtensionIndicator, PTASummary, PTESummay, ApplicationStatus, PatentNumber, Examiner, ArtUnit, Attorney, '\n",
    "'PracticeCategory, SmallEntity, First_toFile, FileLocation, PGPUBNumber, WIPONumber, Inventor, ForeignParent, '\n",
    "'City, Region, Country, PostalCode, Correspondence, Address, has_child, has_parent, is_continuation_type, has_class, '\n",
    "'has_subclass, is_subclass, has_code, has_pta_event, is_extension, has_start, has_pta_summary, has_pte_summary, '\n",
    "'has_status, has_patent, has_examiner, from_unit, has_attorney, has_practice_category, is_small, follows_ftf, at_location, '\n",
    "'has_PGPUB, has_WIPO, filed_application, has_foreign_parent, from_city, from_region, from_country, in_country, in_city, '\n",
    "'filed_country, in_code, in_region, has_correspondence, has_address)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's head back into **GraphStudio** and check out the Graph we just created. Select the **Patents** graph from the *Switch Graph* dropdown, and navigate to the **Design Schema** tab.\n",
    "\n",
    "<img src=\"images/create_graph_4.png\" width=\"100%\" />\n",
    "\n",
    "Hey, that looks familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to the new Graph\n",
    "\n",
    "Remember that **Token** and **Secret** are stored on the Graph level, so we need to re-authenticate with the new Graph that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capital P this time\n",
    "graphName = \"Patents\"\n",
    "\n",
    "# set the name of the graph that we want to connect to\n",
    "conn.graphname = graphName\n",
    "\n",
    "# create a secret\n",
    "secret = conn.createSecret()\n",
    "# use the secret to get a token\n",
    "authToken = conn.getToken(secret)[0]\n",
    "\n",
    "# connect to graph with token\n",
    "conn = tg.TigerGraphConnection(host=hostName, username=userName, password=password, graphname=graphName, apiToken=authToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mapping\n",
    "\n",
    "Once our schema is loaded on the TigerGraph server, we can begin mapping our data to that schema. The data mapping tells TigerGraph which columns of our input files match to which IDs and attributes within our graph. Just like with our schema, I'll walk through doing this via the GraphStudio interface and via GSQL code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Data in GraphStudio\n",
    "\n",
    "Before we can create our mapping in Graph studio, we need to first upload our data files. Since we're still testing out our mapping, we don't want to go upload all 50gb (~4 hours on free tier) of our data just to find out we need to re-format a field in one of the files and re-upload. We'll use the 10K line files that we created previously. Once we have verified that our mapping and connections look correct, then we can proceed from there. In this case, I would probably move up to 100K lines just to reduce the chance of outliers when we move to the full dataset. Being that some of the files in our dataset contain 100M+ lines, 100K is still a relatively small fraction of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Files\n",
    "\n",
    "The first step is to get our data files on the TigerGraph Solution. We can use the **Add Data File** button to bring up the file upload window.\n",
    "\n",
    "<img src=\"images/data_mapping_1.png\" width=\"500px\" />\n",
    "\n",
    "Use the **Plus Icon** to select the files from your system that you would like to upload. We'll select all of the 10K files that we created earlier.\n",
    "\n",
    "<img src=\"images/data_mapping_2.png\" width=\"500px\" />\n",
    "\n",
    "All of the uploaded files will now be visible on the interface. When you click on a file, the first 10 lines of the file will be displayed in the windows on the right. \n",
    "\n",
    "<img src=\"images/data_mapping_3.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the file preview, we can see come configuration options for the file. \n",
    "\n",
    "<img src=\"images/data_mapping_4.png\" width=\"500px\" />\n",
    "\n",
    "- **File Format** - This specifies the type of file that is being loaded (text or an archive)\n",
    "- **Delimiter** - The character used to separate columns in the input file\n",
    "- **End of line** - Character that is used to mark the end of a line\n",
    "- **Enclosing character** - The character used to surround any fields that may contain a Delimiter\n",
    "- **Has header** - Checkbox indicates that the first line of the file is a header\n",
    "\n",
    "We'll check **Has header** for our files because they all have headers. Some of the files also use quotes as an enclosing character, so we will select that as well. The rest of the values can remain their default. Click the **Add** button to add the data file to the Data Mapping.\n",
    "\n",
    "<img src=\"images/data_mapping_5.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping a Vertex\n",
    "\n",
    "Click the Crossed arrows icon to begin mapping your data file to edges and vertices.\n",
    "\n",
    "<img src=\"images/data_mapping_6.png\" width=\"500px\" />\n",
    "\n",
    "Select the Data File, then the Vertex or Edge that you would like to map it to. This will cause the right panel to populate with a table representing our file and one representing the Graph Aspect that we have selected.\n",
    "\n",
    "We're going to start by mapping the *10K_all_inventors.csv* file to the **Inventor** vertex.\n",
    "\n",
    "<img src=\"images/data_mapping_7.png\" width=\"500px\" />\n",
    "\n",
    "Click the field from the file (on the left) that you would like to map, then select the Id or Attribute that you would like to map to from the Vertex table (on the right).\n",
    "\n",
    "<img src=\"images/data_mapping_8.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Functions\n",
    "\n",
    "If you recall from when we created the schema, we don't have a unique ID for our **Inventor**, so we need to create one. For the purpose of this graph, we're going to assume that there are no Inventors who have the same First, Middle, and Last name and we're going to create a unique Id based on the concatenation of those values.\n",
    "\n",
    "Select the Sigma symbol from the toolbar to bring up the Token function selection window. We're going to use `gsql_concat` with 5 inputs. Why 5 inputs? We want the format of this name to match that of other names in our dataset. In particular the **Examiner Name** from the `all_applicatons.csv` file with is structured: Last,First Middle\n",
    "\n",
    "Our five values that we need to concatenate are going to be the Last name, a comma character, the First name, a space character, and the Middle name.\n",
    "\n",
    "<img src=\"images/data_mapping_9.png\" width=\"500px\" /> \n",
    "<br>\n",
    "<img src=\"images/data_mapping_10.png\" width=\"500px\" />\n",
    "\n",
    "Use the **Add** button to add the Token function to the mapping.\n",
    "\n",
    "We can map the values just like we did with the values from the .csv file. In order to input a static value like the , or space, we can double click on the name of the input that we'd like to set. The output from the Token function can be mapped to the id of the **Inventor** vertex.\n",
    "\n",
    "<img src=\"images/data_mapping_11.png\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping an Edge\n",
    "\n",
    "Mapping a file to an edge is much the same as a **Vertex**. The only distinction here is that you have to specify the **source** and **destination** vertex types. Note that we need to include the **token** function here as well to generate the Id for the **Inventor** vertex.\n",
    "\n",
    "<img src=\"images/data_mapping_12.png\" width=\"700px\" />\n",
    "\n",
    "Something important to note is that if a **vertex does not have any attributes**, but **does have an edge** connected to it, then you o**nly need to map to the edge**. This is because an edge can not exits without a source and destination vertex. If you map an edge to both a source and destination vertex, then if either of those vertices does not exist, they will be created. Creation by this process will set the attributes of the source and destination vertices to their default values as we do not have information for them (remember we only have access to the vertex id when mapping an edge). A vertex with no attributes just has an id, and that would be set from the edge creation, so there is no need to map any data directly to that vertex.\n",
    "\n",
    "For example, the City vertex just has an id, so we only need to map that via the *from_city* edge.\n",
    "\n",
    "<img src=\"images/data_mapping_13.png\" width=\"100%\" />\n",
    "\n",
    "<img src=\"images/data_mapping_14.png\" width=\"100%\" />\n",
    "\n",
    "The **City** vertex only has id available for mapping and that's already mapped by the *from_city* edge mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping in GSQL\n",
    "\n",
    "Just like with our Schema, anything that can be set through the GraphStudio interface can be expressed as GSQL code.\n",
    "\n",
    "As for our data mapping, the end result of what we were doing in GraphStudio is a **Loading Job**. Clicking **Publish Data Mapping** in the interface will cause the TigerGraph server to generate GSQL **Loading Jobs** based off of your data mapping. Doing this through GSQL, we'll just directly write the **Loading Jobs**.\n",
    "\n",
    "Here's what the **Loading Job** for the `all_inventors` mapping looks like.\n",
    "\n",
    "```\n",
    "CREATE LOADING JOB load_all_inventors FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO VERTEX Inventor VALUES(gsql_concat($1,\",\",$3,\" \",$2), $1, $2, $3) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE filed_application VALUES($0, gsql_concat($1,\",\",$3,\" \",$2), $4) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE from_city VALUES(gsql_concat($1,\",\",$3,\" \",$2), $5) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE from_region VALUES(gsql_concat($1,\",\",$3,\" \",$2), $6) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE from_country VALUES(gsql_concat($1,\",\",$3,\" \",$2), $7) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a Loading Job\n",
    "\n",
    "Let's break down that loading job so that it makes a little more sense. \n",
    "\n",
    "`CREATE LOADING JOB <LOADING_JOB_NAME> FOR GRAPH <GRAPH_NAME> {`\n",
    "\n",
    "- `CREATE` - We're adding something to the solution\n",
    "- `LOADING JOB` - It's an loading job that's being added\n",
    "- `<LOADING_JOB_NAME>` - The name you want to give to your loading job\n",
    "- `FOR GRAPH` - The next input will specify which graph the job is for\n",
    "- `<GRAPH_NAME>` - The name of the graph that the loading job is for\n",
    "\n",
    "```\n",
    "DEFINE FILENAME <FILE_VARIABLE_NAME>;\n",
    "LOAD <FILE_VARIABLE_NAME>\n",
    "  TO VERTEX|EDGE <VERTEX_TYPE>|<EDGE_TYPE> VALUES(<COLUMN_NUMBER>, ...)\n",
    "```\n",
    "\n",
    "- `DEFINE` - Defining a Variable\n",
    "- `FILENAME` - The type of variable being defined\n",
    "- `<FILE_VARIABLE_NAME>` - The name of the variable that will represent our input file\n",
    "- `LOAD` - Specify that the next input is the file that we will be loading\n",
    "- `<FILE_VARIABLE_NAME>` - The file variable that we are loading\n",
    "- `TO` - The next input is a vertex type or edge type that the loading job will apply to\n",
    "- `VERTEX|EDGE` - Specify if the Job is loading to a Vertex or Edge\n",
    "- `<VERTEX_TYPE>|<EDGE_TYPE>` - The name of the vertex type or edge type being loaded into\n",
    "- `VALUES` - The next input contains the Column Numbers in order of the fields they represent\n",
    "\n",
    "**Values Layout**\n",
    "\n",
    "**Vertex**\n",
    "`VALUES(PRIMARY_ID, ATTRIBUTE_1, ATTRIBUTE_2, ...)`\n",
    "\n",
    "**Edge**\n",
    "`VALUES(SOURCE_ID, DESTINATION_ID, ATTRIBUTE_1, ATTRIBUTE_2, ...)`\n",
    "\n",
    "In the Values section we're using $0, $1, $2, etc. to denote which column's data should be loaded into which value. **$0** represents the first column in the input data file, **$1** the second column, and so on. Additionally, token functions are represented as such `token_function_name(input1, input2, ...)`.\n",
    "\n",
    "Additionally we specify any additional options for the loading job after a `USING` statement. For a full list of options check the [Loading Job Documentation](https://docs.tigergraph.com/dev/gsql-ref/ddl-and-loading/creating-a-loading-job#create-loading-job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing a Loading Job\n",
    "\n",
    "Again we'll use our pyTigerGraph connection to execute the GSQL that will create our Loading Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Using graph 'Patents'\\nSuccessfully created loading jobs: [load_all_inventors].\\nSuccessfully created loading jobs: [load_application_data].\\nSuccessfully created loading jobs: [load_attorney_agent].\\nSuccessfully created loading jobs: [load_continuity_children].\\nSuccessfully created loading jobs: [load_continuity_parents].\\nSuccessfully created loading jobs: [load_correspondence_address].\\nSuccessfully created loading jobs: [load_event_codes].\\nSuccessfully created loading jobs: [load_foreign_priority].\\nSuccessfully created loading jobs: [load_pat_term_adj].\\nSuccessfully created loading jobs: [load_transactions].\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.gsql('''\n",
    "USE GRAPH Patents\n",
    "CREATE LOADING JOB load_all_inventors FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO VERTEX Inventor VALUES(gsql_concat($1,\",\",$3,\" \",$2), $1, $2, $3) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE filed_application VALUES($0, gsql_concat($1,\",\",$3,\" \",$2), $4) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE from_city VALUES(gsql_concat($1,\",\",$3,\" \",$2), $5) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE from_region VALUES(gsql_concat($1,\",\",$3,\" \",$2), $6) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE from_country VALUES(gsql_concat($1,\",\",$3,\" \",$2), $7) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_application_data FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO VERTEX Application VALUES($0, $1, $7, $8, $19) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_examiner VALUES($0, $3) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE from_unit VALUES($3, $4) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_class VALUES($0, $5) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_subclass VALUES($0, $6) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE is_subclass VALUES($6, $5) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_status VALUES($0, $9, $10) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_patent VALUES($0, $17, $18) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE is_small VALUES($0, $20) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE follows_ftf VALUES($0, $21) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE at_location VALUES($0, $11, $12) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_PGPUB VALUES($0, $13, $14) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_attorney_agent FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO VERTEX Attorney VALUES($5, $0, $2, $1, $3, $4) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_attorney VALUES($7, $5) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_practice_category VALUES($5, $6) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_continuity_children FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO EDGE has_child VALUES($0, $1, $2) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_continuity_parents FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO EDGE has_parent VALUES($0, $1, $2) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE is_continuation_type VALUES($0, $3) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_correspondence_address FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO VERTEX Correspondence VALUES($9, $1, $9) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO VERTEX Address VALUES(gsql_concat($2,$3,$4), $2, $3, $4) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_address VALUES(gsql_concat($2,$3,$4), $9) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE in_code VALUES($9 Correspondence, $6 PostalCode) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE in_city VALUES($9 Correspondence, $5 City) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE in_region VALUES($9 Correspondence, $7 Region) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE in_country VALUES($9 Correspondence, $8 Country) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE in_code VALUES(gsql_concat($2,$3,$4) Address, $6 PostalCode) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE in_city VALUES(gsql_concat($2,$3,$4) Address, $5 City) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE in_region VALUES(gsql_concat($2,$3,$4) Address, $7 Region) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE in_country VALUES(gsql_concat($2,$3,$4) Address, $8 Country) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_event_codes FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO VERTEX EventCode VALUES($0, $1) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_foreign_priority FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO EDGE has_foreign_parent VALUES($0, $1, $2) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE filed_country VALUES($1, $3) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_pat_term_adj FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO VERTEX PTAEvent VALUES(gsql_uuid_v4($1), $3, $4, $5) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE has_pta_event VALUES($0, gsql_uuid_v4($1), $2) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "      LOAD MyDataSource TO EDGE is_extension VALUES(gsql_uuid_v4($1), $7) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "CREATE LOADING JOB load_transactions FOR GRAPH Patents {\n",
    "      DEFINE FILENAME MyDataSource;\n",
    "      LOAD MyDataSource TO EDGE has_code VALUES($0, $1, $2) USING SEPARATOR=\",\", HEADER=\"false\", EOL=\"\\n\", QUOTE=\"double\";\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Now that the loading jobs have been created, we can begin actually loading in data. We'll be stepping away from the GSQL heavy work that we've been using so far and switch back to more python oriented code for loading.\n",
    "\n",
    "First, we load the data file into a variable.\n",
    "\n",
    "`uploadFile()` requires 3 inputs:\n",
    "- `filePath` - The actual data file to load\n",
    "- `fileTag` - This is the name of the variable that the file will correspond to in the loading job. If you remember, we're using `MyDataSource` as our FILENAME variable in the loading jobs.\n",
    "- `jobName` - The name of the corresponding loading job to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventors_file = './processed_data/10K_all_inventors.csv'\n",
    "results = conn.uploadFile(inventors_file, fileTag='MyDataSource', jobName='load_all_inventors')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic Loading\n",
    "\n",
    "Loading one file at a time is great and all, but let's let Python handle the heavy lifting. We'll set up a list containing tuples representing the mapping between loading job and file being loaded.\n",
    "\n",
    "Then we can just iterate through that list and run the corresponding loading jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_list = [\n",
    "    ('load_all_inventors', './processed_data/10K_all_inventors.csv'),\n",
    "    ('load_application_data', './processed_data/10K_application_data.csv'),\n",
    "    ('load_attorney_agent', './processed_data/10K_attorney_agent.csv'),\n",
    "    ('load_continuity_children', './processed_data/10K_continuity_children.csv'),\n",
    "    ('load_continuity_parents', './processed_data/10K_continuity_parents.csv'),\n",
    "    ('load_correspondence_address', './processed_data/10K_correspondence_address.csv'),\n",
    "    ('load_event_codes', './processed_data/10K_event_codes.csv'),\n",
    "    ('load_foreign_priority', './processed_data/10K_foreign_priority.csv'),\n",
    "    ('load_pat_term_adj', './processed_data/10K_pat_term_adj.csv'),\n",
    "    ('load_transactions', './processed_data/10K_transactions.csv')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in loading_list:\n",
    "    print(conn.uploadFile(job[1], fileTag='MyDataSource', jobName=job[0], timeout=600000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And following that giant wall of text, our data has been loaded. Let's see for sure what was loaded in. (These numbers might take a minute to stabilize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex Counts\n",
      "{'ForeignParent': 9897, 'Inventor': 9344, 'FileLocation': 14, 'PGPUBNumber': 352, 'SmallEntity': 5, 'PracticeCategory': 3, 'ExtensionIndicator': 2, 'ApplicationStatus': 45, 'PTASummary': 0, 'PTESummay': 0, 'City': 2931, 'Region': 66, 'Application': 44356, 'Correspondence': 7, 'USPCSubclass': 1615, 'Country': 142, 'ContinuationType': 7, 'PostalCode': 71, 'USPCClass': 403, 'Address': 92, 'EventCode': 2166, 'PTAEvent': 29998, 'First_toFile': 3, 'PatentNumber': 7452, 'ArtUnit': 641, 'WIPONumber': 0, 'Examiner': 2084, 'Attorney': 5667}\n",
      "Edge Counts\n",
      "{'has_child': 9924, 'reverse_has_child': 9924, 'has_parent': 9913, 'reverse_has_parent': 9913, 'is_continuation_type': 9138, 'has_class': 10000, 'has_subclass': 10000, 'is_subclass': 6757, 'reverse_is_subclass': 6757, 'has_code': 7641, 'has_pta_event': 9999, 'is_extension': 10000, 'has_start': 0, 'reverse_has_start': 0, 'has_pta_summary': 0, 'has_pte_summary': 0, 'has_status': 9997, 'has_patent': 7452, 'has_examiner': 10000, 'from_unit': 2706, 'has_attorney': 10000, 'has_practice_category': 5628, 'is_small': 10000, 'follows_ftf': 7428, 'at_location': 8193, 'has_PGPUB': 352, 'has_WIPO': 0, 'filed_application': 9995, 'has_foreign_parent': 9998, 'from_city': 5419, 'from_region': 1614, 'from_country': 5512, 'filed_country': 9897, 'has_correspondence': 0, 'has_address': 7, 'in_code': 99, 'in_region': 99, 'in_country': 99, 'in_city': 99}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vertex Counts\")\n",
    "print(conn.getVertexCount(\"*\"))\n",
    "print(\"Edge Counts\")\n",
    "print(conn.getEdgeCount(\"*\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
